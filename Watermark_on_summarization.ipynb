{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qZ1hRuyX52fG",
        "s1aioKnSLm68"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# load summarization model"
      ],
      "metadata": {
        "id": "2qdISDcVuDpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load model"
      ],
      "metadata": {
        "id": "qZ1hRuyX52fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch transformers\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model.config.output_scores = True\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "import random"
      ],
      "metadata": {
        "id": "KI742etVuG-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# greedy search\n",
        "model.config.num_beams = 1"
      ],
      "metadata": {
        "id": "dp28iemDAgha"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.length_penalty = 1.0"
      ],
      "metadata": {
        "id": "_5j6CFepBZhH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.early_stopping = False"
      ],
      "metadata": {
        "id": "wPHAvAkmInTK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"LET’S GET THIS out of the way: Dua Lipa is finishing her` third album. It’s due for release in 2024 and, despite the trend of musicians announcing and delaying records for years, Lipa will almost certainly meet her deadline. It’s funny to think of a pop star — or any successful young artist — as just another striving professional. But at 27, Lipa has already become the kind of multihyphenate entrepreneur who not only finishes her assignments on time but discusses strategy and efficiency with the clarity of a company founder delivering a TED Talk. “If I wasn’t as organized as I am, I would be a mess right now,” she says when we meet one drizzly May afternoon in London. The singer had asked one of her favorite restaurants, Sushi on Jones, hidden on the second floor of a King’s Cross concert venue, to open before dinner so we could have the place to ourselves, then arrived 10 minutes early to make sure everything was as planned.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\",add_special_tokens=False)\n",
        "generation_output = model.generate(**inputs, max_new_tokens=20, min_length=20, return_dict_in_generate=True,output_scores=True)"
      ],
      "metadata": {
        "id": "jXVVg2zgwqEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_summary = tokenizer.decode(generation_output.sequences[0] ,skip_special_tokens=True)\n",
        "print(type(partial_summary))\n",
        "print(partial_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdJCl3s5uPeD",
        "outputId": "8f0f073d-b75a-41d7-a0ef-91334d1e0e20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "Dua Lipa is finishing her third album. It’s due for release in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = text\n",
        "input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "generated_ids = model.generate(input_ids, max_new_tokens=3, min_length=3, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC_mfrwtXTEg",
        "outputId": "b44c28ac-e7aa-4cdd-f410-c9414c5a8450"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential sampeling"
      ],
      "metadata": {
        "id": "s1aioKnSLm68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## greedy sequential sampling"
      ],
      "metadata": {
        "id": "gi6TdTSjJWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text sequentially(by choosign max probability)\n",
        "original_text = text\n",
        "partial_summary = generated_summary\n",
        "partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "for i in range(7):\n",
        "  gen_num = 4+i\n",
        "  input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "  generated_ids = model.generate(input_ids, max_new_tokens=gen_num, min_length=gen_num, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "  a = softmax(generated_ids.scores[-2][0],dim=-1)\n",
        "  next = torch.argmax(a).item()\n",
        "  generated_ids.sequences[0][:-2] = partial_summary_ids\n",
        "  generated_ids.sequences[0][-2] = next\n",
        "  partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "  generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "  partial_summary = generated_summary\n",
        "  print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTu9AHxsaUi4",
        "outputId": "3bb05dd3-05ed-498a-a5c2-b44f522a54c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Dua\n",
            "Generated Summary: Dua Lip\n",
            "Generated Summary: Dua Lipa\n",
            "Generated Summary: Dua Lipa is\n",
            "Generated Summary: Dua Lipa is finishing\n",
            "Generated Summary: Dua Lipa is finishing her\n",
            "Generated Summary: Dua Lipa is finishing her third\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## greedy sequential sampling with hard watermark"
      ],
      "metadata": {
        "id": "rFvx35iiUGQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model's vocabulary\n",
        "values = list(tokenizer.get_vocab().values())\n",
        "keys = list(tokenizer.get_vocab().keys())\n",
        "vocab = {}\n",
        "for i in range(len(values)):\n",
        "  vocab[values[i]] = keys[i]"
      ],
      "metadata": {
        "id": "RXE8PUrd5CFp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_watermarked(token_id, gama=0.5):\n",
        "  seed = int(token_id)\n",
        "  v = values.copy()\n",
        "  v = [i-1 for i in v]\n",
        "  random.seed(seed)\n",
        "\n",
        "  # Shuffle the vocabulary to randomize it\n",
        "  random.shuffle(v)\n",
        "\n",
        "  total_tokens = len(v)\n",
        "  green_portion = int(total_tokens * gama)\n",
        "\n",
        "  green = v[:green_portion]\n",
        "  red = v[green_portion:]\n",
        "\n",
        "  return (green, red)"
      ],
      "metadata": {
        "id": "bqf65cNPV55r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = text\n",
        "input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "generated_ids = model.generate(input_ids, max_new_tokens=3, min_length=3, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l69j3l8WHula",
        "outputId": "5ac12ca5-4ba9-4513-a344-2a788de83edf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text sequentially with hard watermark (by choosign max probability)\n",
        "original_text = text\n",
        "partial_summary = generated_summary\n",
        "partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "for i in range(7):\n",
        "  watermark_token = partial_summary_ids[-1]\n",
        "\n",
        "  gen_num = 4+i\n",
        "  input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "  generated_ids = model.generate(input_ids, max_new_tokens=gen_num, min_length=gen_num, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "  logit = generated_ids.scores[-2][0]\n",
        "  indices_to_mask = give_watermarked(watermark_token, gama=0.5)[1]\n",
        "  for index in indices_to_mask:\n",
        "    logit[index] = float('-inf')\n",
        "  a = softmax(logit,dim=-1)\n",
        "  next = torch.argmax(a).item()\n",
        "  generated_ids.sequences[0][:-2] = partial_summary_ids\n",
        "  generated_ids.sequences[0][-2] = next\n",
        "  partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "  generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "  partial_summary = generated_summary\n",
        "  print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "id": "wyc1FmOYUfCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7816fbcb-9cd3-4c17-91e7-d048fd080296"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: DUA\n",
            "Generated Summary: DUA lip\n",
            "Generated Summary: DUA lipo\n",
            "Generated Summary: DUA lipo is\n",
            "Generated Summary: DUA lipo is releasing\n",
            "Generated Summary: DUA lipo is releasing the\n",
            "Generated Summary: DUA lipo is releasing the`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## greedy sequential sampling with soft watermark"
      ],
      "metadata": {
        "id": "iKeVgJjeJ1Tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = text\n",
        "input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "generated_ids = model.generate(input_ids, max_new_tokens=3, min_length=3, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbUYQe74saiq",
        "outputId": "92703e18-1353-4ba2-e586-1b99c3d2bbc3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text sequentially(by choosign max probability)\n",
        "original_text = text\n",
        "partial_summary = generated_summary\n",
        "partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "landa = 10\n",
        "# lambda =\n",
        "for i in range(7):\n",
        "  watermark_token = partial_summary_ids[-1]\n",
        "\n",
        "  gen_num = 4+i\n",
        "  input_text = f\"{original_text} {tokenizer.eos_token} {partial_summary}\"\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "  generated_ids = model.generate(input_ids, max_new_tokens=gen_num, min_length=gen_num, return_dict_in_generate=True,output_scores=True, num_return_sequences=1)\n",
        "  green_indices = [index for index in give_watermarked(watermark_token, gama=0.5)[0]]\n",
        "  mask = torch.zeros_like(generated_ids.scores[-2][0])\n",
        "  mask[green_indices] = 1\n",
        "  generated_ids.scores[-2][0] = generated_ids.scores[-2][0] + (mask * landa)\n",
        "  a = softmax(generated_ids.scores[-2][0],dim=-1)\n",
        "  next = torch.argmax(a).item()\n",
        "  generated_ids.sequences[0][:-2] = partial_summary_ids\n",
        "  generated_ids.sequences[0][-2] = next\n",
        "  partial_summary_ids = generated_ids.sequences[0][:-1]\n",
        "\n",
        "  generated_summary = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=True)\n",
        "  partial_summary = generated_summary\n",
        "  print(\"Generated Summary:\", generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeDycq16J5Qm",
        "outputId": "3de81cca-01a5-4ee8-c213-2e31b1dee64a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: DUA\n",
            "Generated Summary: DUA lip\n",
            "Generated Summary: DUA lipo\n",
            "Generated Summary: DUA lipo is\n",
            "Generated Summary: DUA lipo is releasing\n",
            "Generated Summary: DUA lipo is releasing the\n",
            "Generated Summary: DUA lipo is releasing the`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy calculation"
      ],
      "metadata": {
        "id": "puGyLiy3yy5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Entropy comparison only makes sense for similar z\n",
        "# Larger z -> smaller spike, however p having value more than 1/z effect more\n",
        "# so better for comparison\n",
        "\n",
        "def calculate_spike(P, z):\n",
        "  return torch.sum(P / (1 + z * P)).item() # returns a number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWYvEfDMy2ze",
        "outputId": "2b115958-5569-4da6-e121-691c93fa5207"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6667, 1.0000, 1.2000, 1.3333])\n",
            "4.200000286102295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate z-score(& p-value)"
      ],
      "metadata": {
        "id": "tiEZ7p3xkZM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def calculate_z_score(green_tokens, text_length, gama=0.5):\n",
        "  # z-score\n",
        "  z_score = (green_tokens - (gama*text_length)) / (np.sqrt(text_length*gama*(1-gama)))\n",
        "\n",
        "  p_value = 1 * (1 - stats.norm.cdf(abs(z_score)))  # One-tailed test\n",
        "\n",
        "  return (z_score, p_value)"
      ],
      "metadata": {
        "id": "xvPf-P0Wy2_N"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_z_score(128, 200, gama=0.5)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LObzg4WrpmGU",
        "outputId": "75e32f96-06a7-4a21-e9bc-83cc30870aa2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.959797974644666"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# divide(& conqure)"
      ],
      "metadata": {
        "id": "KPnQmkJ8qPMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Picking the next\n",
        "from torch.nn.functional import softmax\n",
        "f = ''\n",
        "for i in range(20):\n",
        "  a = softmax(generation_output.scores[i][0],dim=-1)\n",
        "  next = torch.argmax(a).item()\n",
        "  print(torch.tensor([next]))\n",
        "  next = tokenizer.decode(next)"
      ],
      "metadata": {
        "id": "nUgFOsO8N1mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For soft\n",
        "t = generation_output.scores[6][0]\n",
        "print(t)\n",
        "l = [0,1,2]\n",
        "a = 10\n",
        "mask = torch.zeros_like(t)\n",
        "mask[l] = 1\n",
        "result = t + (mask*a)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwm-QDzhO_ut",
        "outputId": "d05e4da8-b5da-4dc9-cf49-0e485269be89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-21.7627, -12.7303,     -inf,  ..., -12.8755, -13.3142, -12.9490])\n",
            "tensor([-11.7627,  -2.7303,     -inf,  ..., -12.8755, -13.3142, -12.9490])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For hard\n",
        "t = generation_output.scores[5][0]\n",
        "print(t)\n",
        "indices_to_mask = [1,3]\n",
        "for index in indices_to_mask:\n",
        "  t[index] = float('-inf')\n",
        "s = softmax(t,dim=-1)\n",
        "print(s)\n",
        "print(s[1])\n",
        "print(s[3])"
      ],
      "metadata": {
        "id": "EPZNuE1H2ULd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cb71c0-841b-4859-bc3a-61b2a4941897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-19.6683, -13.2819,     -inf,  ..., -13.3650, -13.4223, -13.4817])\n",
            "tensor([2.8720e-09, 0.0000e+00, 0.0000e+00,  ..., 1.5691e-06, 1.4818e-06,\n",
            "        1.3963e-06])\n",
            "tensor(0.)\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# handling special tokens\n",
        "special_tokens = tokenizer.special_tokens_map.values()\n",
        "token_to_check = \"[SEP]\"\n",
        "a = softmax(generation_output.scores[13][0],dim=-1)\n",
        "next = torch.argmax(a).item()\n",
        "next = tokenizer.decode(next)\n",
        "token_to_check = next\n",
        "is_special_token = token_to_check in special_tokens\n",
        "print(\"Special Tokens:\", special_tokens)\n",
        "print(f\"'{token_to_check}' is a special token: {is_special_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMv6vNe_-ddw",
        "outputId": "153455bb-2332-421d-dc60-5acb5718fb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special Tokens: dict_values(['<s>', '</s>', '<unk>', '</s>', '<pad>', '<s>', '<mask>'])\n",
            "' 2024' is a special token: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "ua4UgCIS2oGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"xsum\")"
      ],
      "metadata": {
        "id": "MZioBG493JVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example in dataset[\"test\"]:\n",
        "    input_text = example[\"document\"]\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate a summary\n",
        "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode and print the generated summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    print(\"Original Text:\", input_text)\n",
        "    print(\"Generated Summary:\", summary)\n",
        "    break"
      ],
      "metadata": {
        "id": "QCu6yP2o4Ku_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy of a sentence"
      ],
      "metadata": {
        "id": "XH5XCw79bT9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsmJ7xTxbX2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ybM4im0bYAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Watermark"
      ],
      "metadata": {
        "id": "i839szMF51wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create hash & devide vocab"
      ],
      "metadata": {
        "id": "h_mtbNP27oPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Your input text goes here.\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "v = values.copy()\n",
        "\n",
        "# Seed a random number generator with the hash\n",
        "seed = int(input_ids[0, 3])\n",
        "random.seed(seed)\n",
        "\n",
        "# Shuffle the vocabulary to randomize it\n",
        "random.shuffle(v)\n",
        "\n",
        "# Calculate the size of each set\n",
        "total_tokens = len(v)\n",
        "half_tokens = total_tokens // 2\n",
        "\n",
        "# Divide the vocabulary into two sets with equal sizes\n",
        "green = v[:half_tokens]\n",
        "red = v[half_tokens:]\n",
        "\n",
        "# Print some statistics\n",
        "print(\"Total tokens in vocabulary:\", total_tokens)\n",
        "print(\"Tokens in Set 1:\", len(green))\n",
        "print(\"Tokens in Set 2:\", len(red))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx0z95Vj8ek5",
        "outputId": "7a13351f-ded7-4a9e-b83b-55a6bacb94a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in vocabulary: 50265\n",
            "Tokens in Set 1: 25132\n",
            "Tokens in Set 2: 25133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "input_text = \"This is an example input.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output_ids = model.generate(input_ids)\n",
        "\n",
        "# Decode the generated output\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Get the output scores\n",
        "output_scores = model(input_ids).logits\n",
        "\n",
        "print(\"Generated Output:\", output_text)\n",
        "print(\"Output Scores:\", output_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5Uy5VsaOuat",
        "outputId": "5f81bfca-d0a2-45e1-b3a8-f93bf97cdf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Output: This is an example input. Use this input to help people understand how to use this code. Use the code to help others understand how this code can be used to help them with their own code. The code can also be used as an example to help other people understand their code.\n",
            "Output Scores: tensor([[[11.1690,  0.5774,  2.6892,  ...,  0.2707,  0.1449,  0.0438],\n",
            "         [11.1690,  0.5774,  2.6892,  ...,  0.2707,  0.1449,  0.0438],\n",
            "         [-5.0483, -0.0825,  1.7771,  ..., -0.0807, -0.0811, -0.0668],\n",
            "         ...,\n",
            "         [-5.4172,  0.2053,  4.9584,  ...,  0.3211, -0.1325,  0.3382],\n",
            "         [-7.1791, -0.1360,  3.4101,  ...,  0.2763, -0.2612,  0.3939],\n",
            "         [-1.9115, -0.3598, 11.5676,  ...,  0.2852, -0.5289, -0.1428]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t81cip0yHJUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}