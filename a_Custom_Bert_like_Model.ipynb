{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YoUBCS_iZ3Hw",
        "IRE1ILYnaX0f",
        "Qmley93CaqPc",
        "lKNUkMYHcL7z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "YoUBCS_iZ3Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "46pYjE5yZ5Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\")\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\", output_hidden_states=True)"
      ],
      "metadata": {
        "id": "P_oLpBHAaFLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing data"
      ],
      "metadata": {
        "id": "IRE1ILYnaX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix of size (num_layers, vocab_size, hidden_size)\n",
        "\n",
        "num_layers = 13\n",
        "# including the embedding layer\n",
        "vocab_size = 42000\n",
        "hidden_size = 768\n",
        "\n",
        "#tokens_hidden_states = torch.zeros(num_layers, vocab_size, hidden_size)"
      ],
      "metadata": {
        "id": "DpN-6NNVaY--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a matrix of all hidden_state outputs for all tokens\n",
        "# tokens_hidden_states[layer_num, token_id] = hidden_states[layer_num]\n",
        "for token_id in range(vocab_size):\n",
        "  input_tensor = torch.tensor([[token_id]])\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "    hidden_states = outputs.hidden_states\n",
        "  for layer_num in range(len(hidden_states)):\n",
        "    tokens_hidden_states[layer_num, token_id] = hidden_states[layer_num]"
      ],
      "metadata": {
        "id": "yaTjcASVaawj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cXV1X2Mpaeng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/My Drive/matrix weights/tokens_hidden_states.pt'\n",
        "tokens_hidden_states = torch.load(save_path)"
      ],
      "metadata": {
        "id": "3mG8vDiMahwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# createing a dataset instance to train the model\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import pandas as pd\n",
        "token_ids = [{'id':x} for x in range(vocab_size)]\n",
        "\n",
        "tokens_dataset = Dataset.from_pandas(pd.DataFrame(data= token_ids))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    tokens_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")"
      ],
      "metadata": {
        "id": "nzJPwMYCalp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Qmley93CaqPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Jalal_Bert(PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.bert = BertModel(config)\n",
        "\n",
        "    base_model = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-zwnj-base\", output_hidden_states=True)\n",
        "\n",
        "    self.bert.embeddings = base_model.embeddings\n",
        "    self.bert.encoder.layer[0] = base_model.encoder.layer[0]\n",
        "    self.bert.encoder.layer[1] = base_model.encoder.layer[4]\n",
        "    self.bert.encoder.layer[2] = base_model.encoder.layer[8]\n",
        "    self.bert.encoder.layer[3] = base_model.encoder.layer[11]\n",
        "\n",
        "  def forward_train(self, x, last_layer_number= 4):\n",
        "   with torch.no_grad():\n",
        "    x = self.bert.embeddings(x)\n",
        "   for i in range(last_layer_number):\n",
        "    if i!= last_layer_number-1:\n",
        "      with torch.no_grad():\n",
        "        x = self.bert.encoder.layer[i](x)[0]\n",
        "    else:\n",
        "      x = self.bert.encoder.layer[i](x)[0]\n",
        "   return x\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    return self.bert(input_ids)"
      ],
      "metadata": {
        "id": "WPrcVIboarHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the desired config\n",
        "model_config = model.config\n",
        "model_config.num_hidden_layers = 4\n",
        "model_config.output_hidden_states = True"
      ],
      "metadata": {
        "id": "eeJGtF9CbFAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = Jalal_Bert(model_config)"
      ],
      "metadata": {
        "id": "37wmFuddbMjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "input_tensor = torch.tensor([[2], [5]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n",
        "    hidden_states = outputs.hidden_states\n",
        "    h = hidden_states[1]\n",
        "    H = j.forward_train(input_tensor, 1)\n",
        "\n",
        "print(torch.allclose(h, H))  # Check if the outputs are similar"
      ],
      "metadata": {
        "id": "4oTI4_6HbPKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "lKNUkMYHcL7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop (based on number of the layer_num, and criterion)\n",
        "\n",
        "from tqdm import tqdm\n",
        "def train_loop(layer_num, epochs, criterion, optimizer):\n",
        "  # things to keep track of\n",
        "  losses = []\n",
        "\n",
        "  # dic for mapping layer_num with original model\n",
        "  layer_map = {1: 1, 2: 5, 3: 9, 4: 12}\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    j.train()\n",
        "    I = 0\n",
        "    # train_loader\n",
        "    for row in tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      I+= 1\n",
        "      ids = row['id']\n",
        "      inputs = row['id'].view(-1, 1)\n",
        "\n",
        "      # jalal\n",
        "      ids = [torch.tensor([[id]]) for id in ids]\n",
        "      H = [j(id, layer_num) for id in ids]\n",
        "      H = torch.stack(H)\n",
        "      H = H.view(len(ids), -1)\n",
        "\n",
        "      # original\n",
        "      original = tokens_hidden_states[layer_map[layer_num], ids, :]\n",
        "\n",
        "      # loss and optimizing\n",
        "      loss = criterion(H, original)\n",
        "      if I%100 == 0:\n",
        "        print(f'epoch: {epoch}, layer_num: {layer_num}, loss: {loss.item()}')\n",
        "      losses.append(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return losses"
      ],
      "metadata": {
        "id": "LjS87b64bbTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameters of training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "Num_epoch = 2\n",
        "optimizer = optim.Adam(jalal.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "DCPwJbDScP1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply training loop\n",
        "layer_nums = [2, 3, 4] # cuz 1 gives same outputs\n",
        "layer_losses = {2:[], 3:[], 4:[]}\n",
        "\n",
        "for layer_num in layer_nums:\n",
        "  losses = train_loop(layer_num, Num_epoch, criterion, optimizer)\n",
        "  layer_losses[layer_num] = losses"
      ],
      "metadata": {
        "id": "ORYrtNzNcYRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}